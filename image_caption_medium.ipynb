{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_caption_medium.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "oQNKbDLyK-LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnHRdYpnKZXg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import plot_model \n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.layers import LSTM , Dropout , Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading the dataset and caption\n",
        "!gdown 1BJgZPftUv0oHAvYqGhVgjE8gX_aAnUY4\n",
        "!gdown 1C8UED1FyiVPorNhUuCD2XFqS3g6jXTSM"
      ],
      "metadata": {
        "id": "ZNd269THLHa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip the images\n",
        "!unzip -q /content/images.zip\n",
        "!rm -rf /content/images.zip"
      ],
      "metadata": {
        "id": "qoacu9SILKON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we will shuffle the photo-caption dataset\n",
        "filename = open('/content/captions.txt','r').readlines()\n",
        "random.shuffle(filename)\n",
        "#after shuffling we will save the .txt file\n",
        "open('/content/captions.txt','w').writelines(filename)"
      ],
      "metadata": {
        "id": "UzrDCdzFLP3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a dictionary to store the photo , caption ... key -> index\n",
        "photo_caption = dict()\n",
        "filename = open('/content/captions.txt','r').read()\n",
        "file_split = filename.split('\\n')\n",
        "#taking first 1000 lines\n",
        "for i ,filename in tqdm(enumerate(file_split[:1000])):\n",
        "  photo_caption_split = filename.split(',')\n",
        "  photo_caption[i] = {\"photo\":photo_caption_split[0],\n",
        "                  'caption':photo_caption_split[1]}"
      ],
      "metadata": {
        "id": "fgybOBsfLSzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "photo_caption[0]"
      ],
      "metadata": {
        "id": "OKn4qTq2Q-P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for feature extraction we will pretrain VGG16 model , you can use resnet , xception pretrain models too.\n",
        "pretrained_models= tf.keras.applications.VGG16(include_top=True,weights=\"imagenet\",pooling='avg')\n",
        "output = Model(pretrained_models.input,pretrained_models.layers[-2].output) #model.layers[-2] will give fully connected layers"
      ],
      "metadata": {
        "id": "9KclGqn2LgjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.summary()"
      ],
      "metadata": {
        "id": "P_1RxEa4LsS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#single photo prediction\n",
        "image = load_img(f'/content/image/Images/1000268201_693b08cb0e.jpg', target_size=(224, 224))\n",
        "image = img_to_array(image)\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "image = preprocess_input(image) #normalize the image\n",
        "feature = output.predict(image, verbose=0)\n",
        "print(f'features : {feature[0]}')\n",
        "print(f'feature len : {len(feature[0])}')"
      ],
      "metadata": {
        "id": "a0yiplXzXnz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#single caption preprocess\n",
        "text = 'A person , riding a bicycle down a red ramp.'\n",
        "print(f'raw text : {text}')\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "print(f'punctuation removed text : {text}')\n",
        "text = text.lower()\n",
        "print(f'lower text : {text}')\n",
        "text = text.split()\n",
        "print(f'split text : {text}')\n",
        "texts = [''.join(t) for t in text if t.isalpha() and len(t) >1]\n",
        "print(f'final text after removing single char & non-alpha char : {texts}')\n",
        "formatted_text = f\"<start> {' '.join(texts)} <end>\"\n",
        "print(f'adding <start> <end> token text : {formatted_text}')"
      ],
      "metadata": {
        "id": "wdeW_YTTYID4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we will preprocess it for the 1000 images and caption\n",
        "photo_cap_preprocess = dict()\n",
        "\n",
        "for i , val in tqdm(photo_caption.items()):\n",
        "  image = load_img(f'/content/image/Images/{val[\"photo\"]}', target_size=(224, 224))\n",
        "  image = img_to_array(image)\n",
        "  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "  image = preprocess_input(image)\n",
        "  feature = output.predict(image, verbose=0)\n",
        "\n",
        "\n",
        "  text = val['caption']\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "  text = text.lower()\n",
        "  text = text.split()\n",
        "  texts = [''.join(t) for t in text if t.isalpha() and len(t) >1]\n",
        "  formatted_text = f\"<start> {' '.join(texts)} <end>\"\n",
        "  \n",
        "\n",
        "  photo_cap_preprocess[i]= {'photo_preprocess' :feature[0],\n",
        "                        'text_preprocess' :formatted_text}"
      ],
      "metadata": {
        "id": "kUFnAGWzLyvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving all text preprocess in a list for tokenizer\n",
        "caption_tokenize = []\n",
        "for i  , val in photo_cap_preprocess.items():\n",
        "  caption_tokenize.append(val['text_preprocess'])"
      ],
      "metadata": {
        "id": "pJ9zngTtNKwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk = Tokenizer()"
      ],
      "metadata": {
        "id": "sp5_V9K4NRA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tk.fit_on_texts(caption_tokenize)"
      ],
      "metadata": {
        "id": "k1GuGq0qNTJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating  text to sequence\n",
        "sentence = []\n",
        "for caption in caption_tokenize:\n",
        "    token_list = tk.texts_to_sequences([caption])[0]\n",
        "    sentence.append(token_list)"
      ],
      "metadata": {
        "id": "Zf0xLSXWNno-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#how text to sequence work \n",
        "tk1 = Tokenizer()\n",
        "ex = ['My name is souvik', 'I live in kolkata']\n",
        "tk1.fit_on_texts(ex)\n",
        "#vocab\n",
        "print(f'vocab : {tk1.word_index}')\n",
        "\n",
        "T2S = tk1.texts_to_sequences(ex)\n",
        "print(f'actual text : {ex}')\n",
        "print(f'text to sequence : {T2S}')"
      ],
      "metadata": {
        "id": "ucK0zhJKbKLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_sentence = max([len(i)for i in sentence])\n",
        "print(f'max len sentence : {max_len_sentence}')"
      ],
      "metadata": {
        "id": "lB7jUV1iNpmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_words = len(tk.word_index)+1\n",
        "print(f'VOCAB : {total_words}')"
      ],
      "metadata": {
        "id": "INwUWKcQNr4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now the most intersting part sequence create , I have explained it in my blog , do check it out !\n",
        "def sequence_create(max_len,total_words):\n",
        "  the_sequence = [] \n",
        "  for j , vals in photo_cap_preprocess.items():\n",
        "      token_list = tk.texts_to_sequences([vals['text_preprocess']])[0]\n",
        "      for i in range(1, len(token_list)):\n",
        "        input = token_list[:i]\n",
        "        output = token_list[i]\n",
        "        input = pad_sequences([input], maxlen=max_len-1)[0]\n",
        "        output = to_categorical([output], num_classes=total_words)[0]\n",
        "        the_sequence.append([photo_cap_preprocess[j]['photo_preprocess'],input,output])\n",
        "  return the_sequence"
      ],
      "metadata": {
        "id": "b4kNVQz0Nt4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = sequence_create(max_len_sentence,total_words)"
      ],
      "metadata": {
        "id": "OjzNMd6kN0rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples[0][0] #photo feature"
      ],
      "metadata": {
        "id": "P3GCwwH9hXhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples[0][1] #caption preprocess"
      ],
      "metadata": {
        "id": "DN5JRpqFhdVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples[0][2] #output"
      ],
      "metadata": {
        "id": "uJO6ZQoVhkLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a generator to process single batch at a time , explained it in my blog\n",
        "def generator(samples, batch_size=256,shuffle_data=True):\n",
        "    num_samples = len(samples)\n",
        "    while True: \n",
        "        for offset in range(0, num_samples, batch_size):\n",
        "          \n",
        "            batch_samples = samples[offset:offset+batch_size]\n",
        "\n",
        "            \n",
        "            X_train = []\n",
        "            y_train = []\n",
        "            x_train = []\n",
        "\n",
        "           \n",
        "            for batch_sample in batch_samples:\n",
        "                # print(batch_sample)\n",
        "                img_name1 = batch_sample[0]\n",
        "              \n",
        "                x = batch_sample[1]\n",
        "                y = batch_sample[2]\n",
        "                \n",
        "            \n",
        "                X_train.append(img_name1)\n",
        "                y_train.append(y)\n",
        "                x_train.append(x)\n",
        "\n",
        "            X_train = np.asarray(X_train)\n",
        "      \n",
        "            y_train = np.asarray(y_train)\n",
        "            x_train = np.asarray(x_train)\n",
        "\n",
        "            yield [X_train , x_train] , y_train"
      ],
      "metadata": {
        "id": "pX6YzpsNN4CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#building the lstm model\n",
        "\n",
        "def build_lstm():\n",
        "  inputs1 = tf.keras.Input(shape=(4096,))\n",
        "  fe2 = Dense(256, activation='relu')(inputs1)\n",
        "  dp = Dropout(0.3)(fe2)\n",
        "\n",
        "  input = Input(shape=(max_len_sentence-1,))\n",
        "  mod = Embedding(total_words, 512 , mask_zero=True)(input)\n",
        "  mod = LSTM(256,return_sequences=True )(mod)\n",
        "  dp1 = Dropout(0.3)(mod)\n",
        "  mod = LSTM(256)(dp1)\n",
        "\n",
        "  added = layers.add([dp,mod])\n",
        "\n",
        "  decoder2 = Dense(256, activation='relu')(added)\n",
        "\n",
        "  outputs = Dense(total_words, activation='softmax')(decoder2)\n",
        "  model = Model([inputs1,input],outputs)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=[\"Accuracy\"])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "9aUHf7N8ODzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_lstm()"
      ],
      "metadata": {
        "id": "yUnEwnaYOGa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = generator(samples,batch_size=1024)"
      ],
      "metadata": {
        "id": "t7wR6M4yOJGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = next(train_data)"
      ],
      "metadata": {
        "id": "dTllKId8OMKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('x_shape: ', x[0].shape)\n",
        "print ('x_shape: ', x[1].shape)\n",
        "print ('labels: ', y)"
      ],
      "metadata": {
        "id": "b2PqkRrNOO0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelcheck  = ModelCheckpoint('caption_best.h5',save_best_only=True,mode='min',monitor='loss')"
      ],
      "metadata": {
        "id": "cZQz0ezhOSfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = len(samples)//1024\n",
        "model.fit(train_data,epochs=100,callbacks=[modelcheck],steps_per_epoch=steps)"
      ],
      "metadata": {
        "id": "rxKdj7EQOWgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = load_model('/content/caption_best.h5')"
      ],
      "metadata": {
        "id": "8iXmJAG6Ogqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/image/Images/3644142276_caed26029e.jpg'\n",
        "\n",
        "im = cv2.imread(path)\n",
        "image1 = load_img(path, target_size=(224, 224))\n",
        "image = img_to_array(image1)\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "image = preprocess_input(image)\n",
        "img2 = output.predict(image, verbose=0)\n",
        "output_word = '<start>'\n",
        "for i in range(max_len_sentence-1): \n",
        "  t_to_seq = tk.texts_to_sequences([output_word])[0]\n",
        "  padding = pad_sequences([t_to_seq],maxlen=max_len_sentence-1)\n",
        "  # print(padding)\n",
        "  pred = np.argmax(best_model.predict([img2,padding]),axis=-1)\n",
        "  # print(pred)\n",
        "  for word, index in tk.word_index.items():\n",
        "        if index == pred:\n",
        "          if word == 'end':\n",
        "            break\n",
        "          output_word += ' ' + word\n",
        "          \n",
        "    \n",
        "plt.imshow(im[:,:,::-1])\n",
        "plt.axis('off')\n",
        "plt.show()        \n",
        "\n",
        "print(f'predicted caption : {output_word[7:]}.')\n"
      ],
      "metadata": {
        "id": "T3SQBudVPOD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_states()"
      ],
      "metadata": {
        "id": "020-IZSBPYUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oAJFaq8ojHj7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}